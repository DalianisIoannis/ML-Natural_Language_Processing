{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRY_FFNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyObQw4gXYNKcGLFlVJfib1a"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQY9SwLkydRz"},"source":["# Γιάννης Δαλιάνης\n","# 1115201700027\n","# Homework 2\n","# Άσκηση 5"]},{"cell_type":"code","metadata":{"id":"yGaeiJ3lyPv0"},"source":["import os\n","import re\n","from google.colab import drive\n","import pandas as pd\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, roc_curve, auc\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import classification_report\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","import random\n","import numpy as np\n","\n","import spacy\n","# nlp = spacy.load('en')\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","from matplotlib import pyplot as plt\n","\n","import torch.nn.functional as F\n","\n","import time\n","\n","!pip install transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import BertModel\n","from transformers import BertTokenizer\n","\n","import torch   \n","from torchtext import data, datasets\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V92N4VW11E0w"},"source":["Drop uneccessary columns."]},{"cell_type":"code","metadata":{"id":"O-2jcppmym1C"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","\n","tweets = pd.read_csv('/content/gdrive/My Drive/DI/runAI2/ex2/SentimentTweets.csv', index_col=0)\n","\n","print(tweets.columns)\n","\n","tweets = tweets.drop(['id', 'date', 'flag', 'user'], axis=1)\n","tweets.rename(columns={'target': 'label'}, inplace = True)\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3mTXQrv0gZu"},"source":["# tweets = tweets.sample(n = 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVlEnWZ41Hvj"},"source":["Clean tweet text, remove stopwords and save to new csv file."]},{"cell_type":"code","metadata":{"id":"SnpvViu01Icg"},"source":["tweets = tweets.apply(lambda x: x.astype(str).str.lower())\n","\n","def cleanText(text):\n","    text = text.str.replace(r'RT[\\s]+', '')                                              # Removing RT\n","    text = text.str.replace(r'&amp;', '&')            # Replace '&amp;' with '&'\n","    text = text.str.replace(r'#.*?(?=\\s|$)', '')                                        # remove hashtags and mentions\n","    text = text.str.replace(r'@.*?(?=\\s|$)', '')\n","    text = text.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)  # remove urls\n","    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n","    text = text.str.replace(r\"\\'re\", \" are\")                                             # Change 're to 'are'\n","    text = text.str.replace(r\"\\'t\", \" not\")                                             # Change 't to 'not'\n","    text = text.str.replace(r\"\\'d\", \" would\")                                           # Change 'd to 'would'\n","    text = text.replace(r'\\\\n',' ', regex=True)                                         # remove newlines and other special characters\n","    text = text.replace(r'\\\\u',' ', regex=True)\n","    text = text.replace(r'\\\\x',' ', regex=True)\n","    text = text.str.replace('\\d+', '')                                                  # remove all numbers\n","    text = text.str.replace(r'\\b(\\w{1,2})\\b', '')                                       # remove words with 2 or 1 letter only\n","    text = text.str.replace('[^\\w\\s]','')                                               # remove punctuations\n","    text = text.apply(lambda x: re.sub(' +', ' ', x))                                   # replace multiple whitespaces\n","    # text = text.str.replace(r'(@.*?)[\\s]', '')        # Remove '@name'\n","    # text = text.str.replace(r'\\s+', ' ').str.strip()  # Remove trailing whitespace\n","    return text\n","\n","def remove_stops(row):\n","  stops = nltk.corpus.stopwords.words(\"english\")\n","  meaningful_words = [w for w in row if w not in stops]\n","  return meaningful_words\n","\n","def rejoin_words(row):\n","  joined_words = ( \" \".join(row))\n","  return joined_words\n","\n","tweets['text'] = cleanText(tweets['text'])\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","# 0 will be for negative and 1 for positive\n","tweets['label'] = tweets['label'].astype(int)\n","tweets['label'].replace(4, 1, inplace=True)\n","\n","# takes some time\n","tweets[\"text\"] = tweets[\"text\"].str.split()\n","tweets['text'] = tweets['text'].apply(lambda x: remove_stops(x))\n","tweets['text'] = tweets['text'].apply(lambda x: rejoin_words(x))\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","tweets.reset_index(drop = True, inplace = True)\n","\n","print(tweets.columns)\n","\n","trainTWEETS, testTWEETS = train_test_split(tweets, test_size=0.2)\n","\n","# trainTWEETS.to_csv(r'/content/gdrive/My Drive/DI/trainTWEETS.csv')\n","# testTWEETS.to_csv(r'/content/gdrive/My Drive/DI/testTWEETS.csv')\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrLGCkWRje6D"},"source":["# SECOND PART\n"]},{"cell_type":"code","metadata":{"id":"q7-sRTjvMpQP"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","start_timeTOTAL = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2i2NiiIV6Uk2"},"source":["Takes time but has awesome results"]},{"cell_type":"code","metadata":{"id":"hZTlumOjYxCy"},"source":["# trainDATA = pd.read_csv('/content/gdrive/My Drive/DI/trainTWEETS.csv', index_col=0)\n","# trainDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","# trainDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hvf09MNOY20L"},"source":["# testDATA = pd.read_csv('/content/gdrive/My Drive/DI/testTWEETS.csv', index_col=0)\n","# testDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","# testDATA\n","\n","# trainDATA = trainDATA.sample(n = 8500)\n","# testDATA = testDATA.sample(n = 600, replace=True)\n","\n","# X = trainDATA.tweet.values\n","# y = trainDATA.label.values\n","# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xz_m1Vdhg6Po"},"source":["### A GPU can be added by going to the menu and selecting:\n","### Runtime -> Change runtime type -> Hardware accelerator: GPU"]},{"cell_type":"code","metadata":{"id":"T4JAGTiRY2vz"},"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"feAU_811L2zw"},"source":["trainDATA = tweets\n","trainDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","\n","trainDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gStMHMljSFyM"},"source":["from gensim.utils import simple_preprocess\n","\n","top_data_df_small = trainDATA\n","\n","top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['tweet']] \n","print(top_data_df_small['tokenized_text'].head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEVu4BQCSnxy"},"source":["def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n","    X_train, X_test, Y_train, Y_test = train_test_split(\n","          top_data_df_small[['label', 'tweet', 'tokenized_text']], \n","          top_data_df_small['label'], \n","          shuffle=shuffle_state,\n","          test_size=test_size, \n","          random_state=15\n","    )\n","    print(\"Value counts for Train sentiments\")\n","    print(Y_train.value_counts())\n","    print(\"Value counts for Test sentiments\")\n","    print(Y_test.value_counts())\n","\n","    X_train = X_train.reset_index()\n","    X_test = X_test.reset_index()\n","    Y_train = Y_train.to_frame()\n","    Y_train = Y_train.reset_index()\n","    Y_test = Y_test.to_frame()\n","    Y_test = Y_test.reset_index()\n","    return X_train, X_test, Y_train, Y_test\n","\n","# Call the train_test_split\n","X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)\n","\n","X_train = X_train.sample(n = 5000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-3lEpi8UaOy"},"source":["# Use cuda if present\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device available for running: \")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cgb5K61MVNlw"},"source":["class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        \n","        # Linear function 1: vocab_size --> 500\n","        self.fc1 = nn.Linear(input_dim, hidden_dim) \n","        # Non-linearity 1\n","        self.relu1 = nn.ReLU()\n","\n","        # Linear function 2: 500 --> 500\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        \n","        # Non-linearity 2\n","        self.relu2 = nn.ReLU()\n","\n","        # Linear function 3 (readout): 500 --> 3\n","        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n","\n","    def forward(self, x):\n","        # Linear function 1\n","        out = self.fc1(x)\n","        # Non-linearity 1\n","        out = self.relu1(out)\n","\n","        # Linear function 2\n","        out = self.fc2(out)\n","        # Non-linearity 2\n","        out = self.relu2(out)\n","\n","        # Linear function 3 (readout)\n","        out = self.fc3(out)\n","\n","        return F.softmax(out, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqOEyi0sVOds"},"source":["from gensim import corpora\n","\n","# Function to return the dictionary either with padding word or without padding\n","def make_dict(top_data_df_small, padding=True):\n","    if padding:\n","        print(\"Dictionary with padded token added\")\n","        review_dict = corpora.Dictionary([['pad']])\n","        review_dict.add_documents(top_data_df_small['tokenized_text'])\n","    else:\n","        print(\"Dictionary without padding\")\n","        review_dict = corpora.Dictionary(top_data_df_small['tokenized_text'])\n","    return review_dict\n","\n","# Make the dictionary without padding for the basic models\n","# review_dict = make_dict(top_data_df_small, padding=False)\n","review_dict = make_dict(top_data_df_small)\n","review_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-KErJ78XFzw"},"source":["VOCAB_SIZE = len(review_dict)\n","NUM_LABELS = 3\n","\n","# Function to make bow vector to be used as input to network\n","def make_bow_vector(review_dict, sentence):\n","    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)\n","    for word in sentence:\n","        vec[review_dict.token2id[word]] += 1\n","    return vec.view(1, -1).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKBKOBJ2XjyN"},"source":["def make_target(label):\n","    if label == 0:\n","        return torch.tensor([0], dtype=torch.long, device=device)\n","    elif label == 1:\n","        return torch.tensor([1], dtype=torch.long, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSfTCUptYfMA"},"source":["VOCAB_SIZE  = len(review_dict)\n","input_dim   = VOCAB_SIZE\n","hidden_dim  = 500\n","output_dim  = 2\n","num_epochs  = 15\n","\n","ff_nn_bow_model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","ff_nn_bow_model.to(device)\n","\n","loss_function = nn.CrossEntropyLoss()\n","optimizer     = optim.SGD(ff_nn_bow_model.parameters(), lr=0.001)\n","\n","losses = []\n","iter = []\n","\n","for epoch in range(num_epochs):\n","    # if (epoch+1) % 5 == 0:\n","    print(\"Epoch completed: \" + str(epoch+1))\n","    train_loss = 0\n","    for index, row in X_train.iterrows():\n","        \n","        # Clearing the accumulated gradients\n","        optimizer.zero_grad()\n","\n","        # Make the bag of words vector for stemmed tokens \n","        bow_vec = make_bow_vector(review_dict, row['tokenized_text'])\n","       \n","        # Forward pass to get output\n","        probs = ff_nn_bow_model(bow_vec)\n","\n","        # Get the target label\n","        target = make_target(Y_train['label'][index])\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = loss_function(probs, target)\n","\n","        # Accumulating the loss over time\n","        train_loss += loss.item()\n","\n","        losses.append( train_loss / len(X_train) )\n","        iter.append( epoch+1 )\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","    train_loss = 0\n","\n","plt.plot(losses, label='Training loss')\n","plt.plot(iter, label='Iters')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wb2Ust7vcUmb"},"source":["bow_ff_nn_predictions = []\n","original_lables_ff_bow = []\n","with torch.no_grad():\n","    for index, row in X_test.iterrows():\n","        bow_vec = make_bow_vector(review_dict, row['tokenized_text'])\n","        probs = ff_nn_bow_model(bow_vec)\n","        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n","        original_lables_ff_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n","\n","print(classification_report(original_lables_ff_bow, bow_ff_nn_predictions))\n","print(\"Accuracy Score -> \",   accuracy_score(   original_lables_ff_bow, bow_ff_nn_predictions))\n","print(\"Precision Score -> \",  precision_score(  original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)\n","print(\"Recall Score -> \",     recall_score(     original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)\n","print(\"F-Measure Score -> \",  f1_score(         original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xd9qwGhWC6kc"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcZsPPjYQ7HT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DxPsI1TQ7J4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1YDQH2EQ8Xg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mBpHPerUQ8v7"},"source":["### A GPU can be added by going to the menu and selecting:\n","### Runtime -> Change runtime type -> Hardware accelerator: GPU"]},{"cell_type":"code","metadata":{"id":"Ogff2KOOQ8v7"},"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HOAQabMQ8v8"},"source":["trainDATA = tweets\n","trainDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","\n","trainDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"doM0Fit0Q8v8"},"source":["from gensim.utils import simple_preprocess\n","\n","top_data_df_small = trainDATA\n","\n","top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['tweet']] \n","print(top_data_df_small['tokenized_text'].head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"As-f9aIPQ8v8"},"source":["def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n","    X_train, X_test, Y_train, Y_test = train_test_split(\n","        top_data_df_small[['label', 'tweet', 'tokenized_text']], \n","        top_data_df_small['label'], \n","        shuffle=shuffle_state,\n","        test_size=test_size, \n","        random_state=15)\n","    print(\"Value counts for Train sentiments\")\n","    print(Y_train.value_counts())\n","    print(\"Value counts for Test sentiments\")\n","    print(Y_test.value_counts())\n","\n","    X_train = X_train.reset_index()\n","    X_test = X_test.reset_index()\n","    Y_train = Y_train.to_frame()\n","    Y_train = Y_train.reset_index()\n","    Y_test = Y_test.to_frame()\n","    Y_test = Y_test.reset_index()\n","    return X_train, X_test, Y_train, Y_test\n","\n","# Call the train_test_split\n","X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)\n","\n","X_train = X_train.sample(n = 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ntc01Gs1Q8v8"},"source":["# Use cuda if present\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device available for running: \")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZMuL86UQ8v8"},"source":["class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        \n","        # Linear function 1: vocab_size --> 500\n","        self.fc1 = nn.Linear(input_dim, hidden_dim) \n","        # Non-linearity 1\n","        # self.relu1 = nn.ReLU()\n","        self.relu1 = nn.Sigmoid()\n","\n","        # Linear function 2: 500 --> 500\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 2\n","        # self.relu2 = nn.ReLU()\n","        self.relu2 = nn.Sigmoid()\n","\n","        # Linear function 3 (readout): 500 --> 3\n","        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n","\n","    def forward(self, x):\n","        # Linear function 1\n","        out = self.fc1(x)\n","        # Non-linearity 1\n","        out = self.relu1(out)\n","\n","        # Linear function 2\n","        out = self.fc2(out)\n","        # Non-linearity 2\n","        out = self.relu2(out)\n","\n","        # Linear function 3 (readout)\n","        out = self.fc3(out)\n","\n","        return F.softmax(out, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"riKvESYGQ8v8"},"source":["from gensim import corpora\n","\n","# Function to return the dictionary either with padding word or without padding\n","def make_dict(top_data_df_small, padding=True):\n","    if padding:\n","        print(\"Dictionary with padded token added\")\n","        review_dict = corpora.Dictionary([['pad']])\n","        review_dict.add_documents(top_data_df_small['tokenized_text'])\n","    else:\n","        print(\"Dictionary without padding\")\n","        review_dict = corpora.Dictionary(top_data_df_small['tokenized_text'])\n","    return review_dict\n","\n","# Make the dictionary without padding for the basic models\n","review_dict = make_dict(top_data_df_small, padding=False)\n","review_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8ixWOBlQ8v8"},"source":["VOCAB_SIZE = len(review_dict)\n","NUM_LABELS = 3\n","\n","# Function to make bow vector to be used as input to network\n","def make_bow_vector(review_dict, sentence):\n","    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)\n","    for word in sentence:\n","        vec[review_dict.token2id[word]] += 1\n","    return vec.view(1, -1).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGS63v_wQ8v8"},"source":["def make_target(label):\n","    if label == 0:\n","        return torch.tensor([0], dtype=torch.long, device=device)\n","    elif label == 1:\n","        return torch.tensor([1], dtype=torch.long, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JojGelRQ8v8"},"source":["VOCAB_SIZE = len(review_dict)\n","\n","input_dim = VOCAB_SIZE\n","hidden_dim = 500\n","output_dim = 2\n","num_epochs = 15\n","\n","ff_nn_bow_model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","ff_nn_bow_model.to(device)\n","\n","loss_function = nn.CrossEntropyLoss()\n","# loss_function = nn.MSELoss()\n","\n","# optimizer = optim.SGD(ff_nn_bow_model.parameters(), lr=0.001)\n","optimizer = AdamW(\n","        ff_nn_bow_model.parameters(),\n","        lr=5e-5,    # Default learning rate\n","        eps=1e-8    # Default epsilon value\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"in59wO_oQ8v8"},"source":["losses = []\n","iter = []\n","# Start training\n","for epoch in range(num_epochs):\n","    # if (epoch+1) % 5 == 0:\n","    print(\"Epoch completed: \" + str(epoch+1))\n","    train_loss = 0\n","    for index, row in X_train.iterrows():\n","        # Clearing the accumulated gradients\n","        optimizer.zero_grad()\n","\n","        # Make the bag of words vector for stemmed tokens \n","        bow_vec = make_bow_vector(review_dict, row['tokenized_text'])\n","       \n","        # Forward pass to get output\n","        probs = ff_nn_bow_model(bow_vec)\n","\n","        # Get the target label\n","        target = make_target(Y_train['label'][index])\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = loss_function(probs, target)\n","        # Accumulating the loss over time\n","        train_loss += loss.item()\n","\n","        losses.append( train_loss / len(X_train) )\n","        iter.append( epoch+1 )\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","    train_loss = 0\n","\n","plt.plot(losses, label='Training loss')\n","plt.plot(iter, label='Iters')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-F5NAXeBQ8v8"},"source":["bow_ff_nn_predictions = []\n","original_lables_ff_bow = []\n","with torch.no_grad():\n","    for index, row in X_test.iterrows():\n","        bow_vec = make_bow_vector(review_dict, row['tokenized_text'])\n","        probs = ff_nn_bow_model(bow_vec)\n","        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n","        original_lables_ff_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n","\n","print(classification_report(original_lables_ff_bow, bow_ff_nn_predictions))\n","print(\"Accuracy Score -> \",   accuracy_score(   original_lables_ff_bow, bow_ff_nn_predictions))\n","print(\"Precision Score -> \",  precision_score(  original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)\n","print(\"Recall Score -> \",     recall_score(     original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)\n","print(\"F-Measure Score -> \",  f1_score(         original_lables_ff_bow, bow_ff_nn_predictions, average='macro')*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLF84H2JQ8v8"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]}]}