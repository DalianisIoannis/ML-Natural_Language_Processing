{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI2_FFNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQY9SwLkydRz"},"source":["# Γιάννης Δαλιάνης\n","# 1115201700027\n","# Homework 2\n","# Άσκηση 5"]},{"cell_type":"code","metadata":{"id":"yGaeiJ3lyPv0"},"source":["import os\n","import re\n","from google.colab import drive\n","import pandas as pd\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, roc_curve, auc\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import classification_report\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","import random\n","import numpy as np\n","\n","import spacy\n","# nlp = spacy.load('en')\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","from matplotlib import pyplot as plt\n","\n","import torch.nn.functional as F\n","\n","import time\n","\n","!pip install transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import BertModel\n","from transformers import BertTokenizer\n","\n","import torch   \n","from torchtext import data, datasets\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V92N4VW11E0w"},"source":["Drop uneccessary columns."]},{"cell_type":"code","metadata":{"id":"O-2jcppmym1C"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","\n","tweets = pd.read_csv('/content/gdrive/My Drive/giannikos/SentimentTweets.csv', index_col=0)\n","\n","print(tweets.columns)\n","\n","tweets = tweets.drop(['id', 'date', 'flag', 'user'], axis=1)\n","tweets.rename(columns={'target': 'label'}, inplace = True)\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3mTXQrv0gZu"},"source":["# tweets = tweets.sample(n = 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVlEnWZ41Hvj"},"source":["Clean tweet text, remove stopwords and save to new csv file."]},{"cell_type":"code","metadata":{"id":"SnpvViu01Icg"},"source":["tweets = tweets.apply(lambda x: x.astype(str).str.lower())\n","\n","def cleanText(text):\n","    text = text.str.replace(r'RT[\\s]+', '')                                              # Removing RT\n","    text = text.str.replace(r'&amp;', '&')            # Replace '&amp;' with '&'\n","    text = text.str.replace(r'#.*?(?=\\s|$)', '')                                        # remove hashtags and mentions\n","    text = text.str.replace(r'@.*?(?=\\s|$)', '')\n","    text = text.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)  # remove urls\n","    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n","    text = text.str.replace(r\"\\'re\", \" are\")                                             # Change 're to 'are'\n","    text = text.str.replace(r\"\\'t\", \" not\")                                             # Change 't to 'not'\n","    text = text.str.replace(r\"\\'d\", \" would\")                                           # Change 'd to 'would'\n","    text = text.replace(r'\\\\n',' ', regex=True)                                         # remove newlines and other special characters\n","    text = text.replace(r'\\\\u',' ', regex=True)\n","    text = text.replace(r'\\\\x',' ', regex=True)\n","    text = text.str.replace('\\d+', '')                                                  # remove all numbers\n","    text = text.str.replace(r'\\b(\\w{1,2})\\b', '')                                       # remove words with 2 or 1 letter only\n","    text = text.str.replace('[^\\w\\s]','')                                               # remove punctuations\n","    text = text.apply(lambda x: re.sub(' +', ' ', x))                                   # replace multiple whitespaces\n","    # text = text.str.replace(r'(@.*?)[\\s]', '')        # Remove '@name'\n","    # text = text.str.replace(r'\\s+', ' ').str.strip()  # Remove trailing whitespace\n","    return text\n","\n","def remove_stops(row):\n","  stops = nltk.corpus.stopwords.words(\"english\")\n","  meaningful_words = [w for w in row if w not in stops]\n","  return meaningful_words\n","\n","def rejoin_words(row):\n","  joined_words = ( \" \".join(row))\n","  return joined_words\n","\n","tweets['text'] = cleanText(tweets['text'])\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","# 0 will be for negative and 1 for positive\n","tweets['label'] = tweets['label'].astype(int)\n","tweets['label'].replace(4, 1, inplace=True)\n","\n","# takes some time\n","tweets[\"text\"] = tweets[\"text\"].str.split()\n","tweets['text'] = tweets['text'].apply(lambda x: remove_stops(x))\n","tweets['text'] = tweets['text'].apply(lambda x: rejoin_words(x))\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","tweets.reset_index(drop = True, inplace = True)\n","\n","print(tweets.columns)\n","\n","trainTWEETS, testTWEETS = train_test_split(tweets, test_size=0.2)\n","\n","trainTWEETS.to_csv(r'/content/gdrive/My Drive/giannikos/trainTWEETS.csv')\n","testTWEETS.to_csv(r'/content/gdrive/My Drive/giannikos/testTWEETS.csv')\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrLGCkWRje6D"},"source":["# SECOND PART\n"]},{"cell_type":"code","metadata":{"id":"q7-sRTjvMpQP"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","start_timeTOTAL = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZTlumOjYxCy"},"source":["trainDATA = pd.read_csv('/content/gdrive/My Drive/giannikos/trainTWEETS.csv', index_col=0)\n","trainDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","trainDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hvf09MNOY20L"},"source":["testDATA = pd.read_csv('/content/gdrive/My Drive/giannikos/testTWEETS.csv', index_col=0)\n","testDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","testDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGLnnfteY2yA"},"source":["trainDATA = trainDATA.sample(n = 8500)\n","# trainDATA = trainDATA.sample(n = 500)\n","# testDATA = testDATA.sample(n = 600, replace=True)\n","\n","\n","X = trainDATA.tweet.values\n","y = trainDATA.label.values\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xz_m1Vdhg6Po"},"source":["### A GPU can be added by going to the menu and selecting:\n","### Runtime -> Change runtime type -> Hardware accelerator: GPU"]},{"cell_type":"code","metadata":{"id":"T4JAGTiRY2vz"},"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGL3FEMlUz1Q"},"source":["def text_preprocessing(text):\n","    # text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","    # text = re.sub(r'&amp;', '&', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, truncation=True)\n","\n","def preprocessing_for_bert(data): # tokenize a set of texts\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=text_preprocessing(sent),  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpsQq3zxUzsq"},"source":["MAX_LEN = 64\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMfSzmdeUzo8"},"source":["# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IhRM6tWDcJ-"},"source":["%%time\n","\n","class BertClassifier(nn.Module):\n","    def __init__(self, freeze_bert=False):\n","        \"\"\"\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 50, 2\n","\n","        # Instantiate BertModel object model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkUI8iMrDcHw"},"source":["def initialize_model(epochs=4):\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSz9_I6_DcFB"},"source":["# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\"\"\"\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After completion of each training epoch measure model performance on our validation set\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\"\"\"\n","    # Put model into evaluation mode. Dropout layers disabled during test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lax2Az_rDcAe"},"source":["set_seed(42)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWknfDJ7W7Xz"},"source":["def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\"\"\"\n","    # Put model into the evaluation mode. dropout layers disabled during test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs\n","    \n","def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1, 0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy*100:.2f}%')\n","    \n","    # Plot ROC AUC\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca5oiaFCDb6o"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tr4AR5vqDb1B"},"source":["testDATA.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oySiO98QDbyS"},"source":["# Run `preprocessing_for_bert` on the test set\n","test_inputs, test_masks = preprocessing_for_bert(testDATA.tweet)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-aAqMEOUzlh"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","output = testDATA[preds==1]\n","\n","print(output)\n","print(classification_report(testDATA['label'], preds))\n","print(\"Accuracy Score -> \",   accuracy_score(   testDATA['label'], preds))\n","print(\"Precision Score -> \",  precision_score(  testDATA['label'], preds, average='macro')*100)\n","print(\"Recall Score -> \",     recall_score(     testDATA['label'], preds, average='macro')*100)\n","print(\"F-Measure Score -> \",  f1_score(         testDATA['label'], preds, average='macro')*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xd9qwGhWC6kc"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCeS9X7IPr9Y"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_wLDiY2PsAf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WTGVVp8PsDH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFvHl301PtNF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z46JNK8ZPtQ2"},"source":["start_timeTOTAL = time.time()\n","trainDATA = pd.read_csv('/content/gdrive/My Drive/giannikos/trainTWEETS.csv', index_col=0)\n","trainDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","trainDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bpGnwAsPtQ2"},"source":["testDATA = pd.read_csv('/content/gdrive/My Drive/giannikos/testTWEETS.csv', index_col=0)\n","testDATA.rename(columns={\"label\": \"label\", \"text\": \"tweet\"}, inplace=True)\n","testDATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35ZuZNtCPtQ2"},"source":["trainDATA = trainDATA.sample(n = 8500)\n","# trainDATA = trainDATA.sample(n = 500)\n","# testDATA = testDATA.sample(n = 600, replace=True)\n","\n","X = trainDATA.tweet.values\n","y = trainDATA.label.values\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TeEQtEtPtQ2"},"source":["# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, truncation=True)\n","\n","def preprocessing_for_bert(data): # tokenize a set of texts\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=text_preprocessing(sent),  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kizAz4oMPtQ2"},"source":["MAX_LEN = 64\n","\n","# Print sentence 0 and its encoded token ids\n","token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n","print('Original: ', X[0])\n","print('Token IDs: ', token_ids)\n","\n","# Run function `preprocessing_for_bert` on the train set and the validation set\n","print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slSf3B1ZPtQ2"},"source":["# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2seSE116PtQ2"},"source":["%%time\n","\n","class BertClassifier(nn.Module):\n","    def __init__(self, freeze_bert=False):\n","        \"\"\"\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H, D_out = 768, 60, 2\n","\n","        # Instantiate BertModel object model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # # Instantiate an one-layer feed-forward classifier\n","        # self.classifier = nn.Sequential(\n","        #     nn.Linear(D_in, H),\n","        #     nn.ReLU(),\n","        #     #nn.Dropout(0.5),\n","        #     nn.Linear(H, D_out)\n","        # )\n","\n","        # # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.Sigmoid(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0xCzYAbKPtQ2"},"source":["def initialize_model(epochs=4):\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(\n","        bert_classifier.parameters(),\n","        lr=5e-5,    # Default learning rate\n","        eps=1e-8    # Default epsilon value\n","    )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xSO9rBrPtQ2"},"source":["# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\"\"\"\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After completion of each training epoch measure model performance on our validation set\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\"\"\"\n","    # Put model into evaluation mode. Dropout layers disabled during test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JGsJr2QPtQ2"},"source":["set_seed(42)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWfiuRSgPtQ2"},"source":["def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\"\"\"\n","    # Put model into the evaluation mode. dropout layers disabled during test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs\n","    \n","def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1, 0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy*100:.2f}%')\n","    \n","    # Plot ROC AUC\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mS8XoYDcPtQ2"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# Evaluate the Bert classifier\n","evaluate_roc(probs, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aBDzurePtQ2"},"source":["testDATA.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEVgbV7BPtQ2"},"source":["# Run `preprocessing_for_bert` on the test set\n","test_inputs, test_masks = preprocessing_for_bert(testDATA.tweet)\n","\n","# Create the DataLoader for our test set\n","test_dataset = TensorDataset(test_inputs, test_masks)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWuk5m2APtQ2"},"source":["# Compute predicted probabilities on the test set\n","probs = bert_predict(bert_classifier, test_dataloader)\n","\n","# Get predictions from the probabilities\n","threshold = 0.5\n","preds = np.where(probs[:, 1] > threshold, 1, 0)\n","\n","output = testDATA[preds==1]\n","\n","print(output)\n","print(classification_report(testDATA['label'], preds))\n","print(\"Accuracy Score -> \",   accuracy_score(   testDATA['label'], preds))\n","print(\"Precision Score -> \",  precision_score(  testDATA['label'], preds, average='macro')*100)\n","print(\"Recall Score -> \",     recall_score(     testDATA['label'], preds, average='macro')*100)\n","print(\"F-Measure Score -> \",  f1_score(         testDATA['label'], preds, average='macro')*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rB-4-ipsPtQ2"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]}]}