{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI2_NOT_FFNN_DONE.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQY9SwLkydRz"},"source":["# Γιάννης Δαλιάνης\n","# 1115201700027\n","# Homework 2\n","# Άσκηση 5"]},{"cell_type":"code","metadata":{"id":"yGaeiJ3lyPv0"},"source":["import os\n","import re\n","from google.colab import drive\n","import pandas as pd\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, roc_curve, auc\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import random\n","import numpy as np\n","\n","import spacy\n","# nlp = spacy.load('en')\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","from matplotlib import pyplot as plt\n","\n","import torch.nn.functional as F\n","\n","import time\n","\n","import torch   \n","from torchtext import data, datasets\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V92N4VW11E0w"},"source":["Drop uneccessary columns."]},{"cell_type":"code","metadata":{"id":"O-2jcppmym1C"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","\n","tweets = pd.read_csv('/content/gdrive/My Drive/giannikos/SentimentTweets.csv', index_col=0)\n","print(tweets.columns)\n","tweets = tweets.drop(['id', 'date', 'flag', 'user'], axis=1)\n","tweets.rename(columns={'target': 'label'}, inplace = True)\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3mTXQrv0gZu"},"source":["# tweets = tweets.sample(n = 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVlEnWZ41Hvj"},"source":["Clean tweet text, remove stopwords and save to new csv file."]},{"cell_type":"code","metadata":{"id":"SnpvViu01Icg"},"source":["tweets = tweets.apply(lambda x: x.astype(str).str.lower())\n","\n","def cleanText(text):\n","    text = text.str.replace(r'&amp;', '&')            # Replace '&amp;' with '&'##################\n","    text = text.str.replace(r'RT[\\s]+', '')                                             # Removing RT\n","    text = text.str.replace(r'#.*?(?=\\s|$)', '')                                        # remove hashtags and mentions\n","    text = text.str.replace(r'@.*?(?=\\s|$)', '')\n","    text = text.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)  # remove urls\n","    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n","    text = text.str.replace(r\"\\'re\", \" are\")                                             # Change 're to 'are'\n","    text = text.str.replace(r\"\\'t\", \" not\")                                             # Change 't to 'not'\n","    text = text.replace(r'\\\\n',' ', regex=True)                                         # remove newlines and other special characters\n","    text = text.replace(r'\\\\u',' ', regex=True)\n","    text = text.replace(r'\\\\x',' ', regex=True)\n","    text = text.str.replace('\\d+', '')                                                  # remove all numbers\n","    text = text.str.replace(r'\\b(\\w{1,2})\\b', '')                                       # remove words with 2 or 1 letter only\n","    text = text.str.replace('[^\\w\\s]','')                                               # remove punctuations\n","    text = text.apply(lambda x: re.sub(' +', ' ', x))                                   # replace multiple whitespaces\n","    return text\n","\n","def remove_stops(row):\n","  stops = nltk.corpus.stopwords.words(\"english\")\n","  meaningful_words = [w for w in row if w not in stops]\n","  return meaningful_words\n","\n","def rejoin_words(row):\n","  joined_words = ( \" \".join(row))\n","  return joined_words\n","\n","tweets['text'] = cleanText(tweets['text'])\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","# 0 will be for negative and 1 for positive\n","tweets['label'] = tweets['label'].astype(int)\n","tweets['label'].replace(4, 1, inplace=True)\n","\n","# takes some time\n","tweets[\"text\"] = tweets[\"text\"].str.split()\n","tweets['text'] = tweets['text'].apply(lambda x: remove_stops(x))\n","tweets['text'] = tweets['text'].apply(lambda x: rejoin_words(x))\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","tweets.reset_index(drop = True, inplace = True)\n","\n","print(tweets.columns)\n","\n","tweets[\"label\"].replace({0: \"neg\", 1: \"pos\"}, inplace=True)\n","\n","trainTWEETS, testTWEETS = train_test_split(tweets, test_size=0.2)\n","\n","trainTWEETS.to_csv(r'/content/gdrive/My Drive/giannikos/trainTWEETS.csv')\n","testTWEETS.to_csv(r'/content/gdrive/My Drive/giannikos/testTWEETS.csv')\n","\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0NhAfXggCmS"},"source":["tmpTRY = pd.read_csv('/content/gdrive/My Drive/giannikos/trainTWEETS.csv', index_col=0)\n","tmpTRY"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uDIW-cimOhJ"},"source":["tmpTRY = pd.read_csv('/content/gdrive/My Drive/giannikos/testTWEETS.csv', index_col=0)\n","\n","# testTRY1 = tmpTRY.copy(deep=True)\n","testTRY2 = tmpTRY.copy(deep=True)\n","testTRY3 = tmpTRY.copy(deep=True)\n","\n","tmpTRY"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7v7kaD6dCIj"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","#Cuda algorithms\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7-sRTjvMpQP"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drtWXIv8OXgd"},"source":["def generate_bigrams(x):\n","    n_grams = set(zip(*[x[i:] for i in range(2)]))\n","    for n_gram in n_grams:\n","        x.append(' '.join(n_gram))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BlSFjmaiSt9"},"source":["def percentage(part, whole):\n","  return 100 * float(part)/float(whole)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgWp-7yMz4Bo"},"source":["if(torch.cuda.is_available()):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBFDs0gjcvnV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bonSWw4abhNA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgcqKo2ycvkn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuX0RBvWLVZs"},"source":["# start_timeTOTAL = time.time()\n","# TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n","# LABEL = data.LabelField(dtype = torch.float, is_target=True, unk_token=None, batch_first=True)\n","# # (None, None) for Unamed 0\n","# fields = [(None, None), ('label', LABEL), ('text',TEXT)]\n","# training_data = data.TabularDataset(\n","#                           path =  '/content/gdrive/My Drive/DI/trainTWEETS.csv',\n","#                           format = 'csv',\n","#                           fields = fields,\n","#                           skip_header = True\n","#                   )\n","# test_data = data.TabularDataset(\n","#                           path =  '/content/gdrive/My Drive/DI/testTWEETS.csv',\n","#                           format = 'csv',\n","#                           fields = fields,\n","#                           skip_header = True\n","#                   )\n","# for i in range(5):\n","#   print(vars(training_data.examples[i]))\n","# train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","# print(f'Number of training examples: {len(train_data)}')\n","# print(f'Number of validation examples: {len(valid_data)}')\n","# print(f'Number of test examples: {len(test_data)}')\n","# #initialize glove embeddings\n","# MAX_VOCAB_SIZE = 25_000\n","# TEXT.build_vocab(\n","#         train_data,\n","#         max_size = MAX_VOCAB_SIZE,\n","#         vectors = \"glove.6B.100d\",\n","#         unk_init = torch.Tensor.normal_,\n","#         # min_freq=4,\n","#     )\n","# LABEL.build_vocab(train_data)\n","# print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","# print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n","# #Commonly used words\n","# print(TEXT.vocab.freqs.most_common(10))\n","# untill=0\n","# for item in TEXT.vocab.stoi.items():\n","#     print(item)\n","#     untill+=1\n","#     if(untill==10):\n","#       break\n","# print(TEXT.vocab.itos[:10])\n","# print(LABEL.vocab.stoi)\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","# BATCH_SIZE = 64\n","# #Load an iterator\n","# train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","#       (train_data, valid_data, test_data), \n","#       batch_size = BATCH_SIZE,\n","#       sort_key = lambda x: len(x.text),\n","#       sort_within_batch=True,\n","#       device = device\n","#     )\n","# class RNN(nn.Module):\n","#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","#                  bidirectional, dropout, pad_idx):\n","#         super().__init__()\n","#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","#         self.rnn = nn.LSTM(embedding_dim, \n","#                            hidden_dim, \n","#                            num_layers=n_layers, \n","#                            bidirectional=bidirectional, \n","#                            dropout=dropout)\n","#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","#         self.dropout = nn.Dropout(dropout)\n","#     def forward(self, text, text_lengths):\n","#         embedded = self.dropout(self.embedding(text))\n","#         #pack sequence\n","#         if(torch.cuda.is_available()):\n","#             packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths = text_lengths.cpu())\n","#         else:\n","#             packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n","#         packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","#         #unpack sequence\n","#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","#         return self.fc(hidden)\n","# #define hyperparameters\n","# INPUT_DIM = len(TEXT.vocab)\n","# EMBEDDING_DIM = 100\n","# N_FILTERS = 100\n","# HIDDEN_DIM = 256\n","# OUTPUT_DIM = 1\n","# N_LAYERS = 2\n","# BIDIRECTIONAL = True\n","# DROPOUT = 0.5\n","# N_EPOCHS = 5\n","# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","# model = RNN(\n","#     INPUT_DIM, \n","#     EMBEDDING_DIM, \n","#     HIDDEN_DIM, \n","#     OUTPUT_DIM, \n","#     N_LAYERS, \n","#     BIDIRECTIONAL, \n","#     DROPOUT, \n","#     PAD_IDX\n","#   )\n","# print(model)\n","# def count_parameters(model):\n","#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","# print(f'The model has {count_parameters(model):,} trainable parameters')\n","# pretrained_embeddings = TEXT.vocab.vectors\n","# print(pretrained_embeddings.shape)\n","# model.embedding.weight.data.copy_(pretrained_embeddings)\n","# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","# model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","# print(model.embedding.weight.data)\n","# optimizer = optim.Adam(model.parameters())\n","# criterion = nn.BCEWithLogitsLoss()\n","# #push to cuda if available\n","# model = model.to(device)\n","# criterion = criterion.to(device)\n","# def binary_accuracy(preds, y):\n","#     #round predictions to the closest integer\n","#     rounded_preds = torch.round(torch.sigmoid(preds))\n","#     correct = (rounded_preds == y).float() \n","#     acc = correct.sum() / len(correct)\n","#     return acc\n","# def train(model, iterator, optimizer, criterion):\n","#     epoch_loss = 0\n","#     epoch_acc = 0\n","#     model.train()  \n","#     for batch in iterator:\n","#         optimizer.zero_grad()   \n","#         text, text_lengths = batch.text\n","#         predictions = model(text, text_lengths).squeeze(1)    # for second\n","#         loss = criterion(predictions, batch.label)        \n","#         acc = binary_accuracy(predictions, batch.label)   \n","#         loss.backward()       \n","#         optimizer.step()      \n","#         epoch_loss += loss.item()  \n","#         epoch_acc += acc.item()    \n","#     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","# def evaluate(model, iterator, criterion):\n","#     epoch_loss = 0\n","#     epoch_acc = 0\n","#     model.eval()\n","#     with torch.no_grad():\n","#         for batch in iterator:\n","#             text, text_lengths = batch.text\n","#             predictions = model(text, text_lengths).squeeze(1)     # 1\n","#             loss = criterion(predictions, batch.label)\n","#             acc = binary_accuracy(predictions, batch.label)\n","#             epoch_loss += loss.item()\n","#             epoch_acc += acc.item()\n","#     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","# best_valid_loss = float('inf')\n","# train_losses = []\n","# val_losses = []\n","# for epoch in range(N_EPOCHS):\n","#     start_time = time.time()\n","#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","#     end_time = time.time()\n","#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","#     if valid_loss < best_valid_loss:\n","#         best_valid_loss = valid_loss\n","#         torch.save(model.state_dict(), 'saved_weights.pt')\n","#     print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\\n')\n","#     train_losses.append(train_loss)\n","#     val_losses.append(valid_loss)\n","# plt.plot(train_losses, label='Training loss')\n","# plt.plot(val_losses, label='Validation loss')\n","# plt.legend()\n","# plt.show()\n","# path='/content/saved_weights.pt'\n","# model.load_state_dict(torch.load(path));\n","# test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n","# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","# def predictFIRST(model, sentence, flag=False):\n","#     model.eval()\n","#     if(flag==False):\n","#       tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","#     else:\n","#         tokenized = sentence\n","#     indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","#     length = [len(indexed)]\n","#     tensor = torch.LongTensor(indexed).to(device)\n","#     tensor = tensor.unsqueeze(1)\n","#     length_tensor = torch.LongTensor(length)\n","#     prediction = torch.sigmoid(model(tensor, length_tensor))\n","#     return prediction.item()\n","# print(\"For text: \"\"\"\"This film is terrible\"\"\"\" ->\", predictFIRST(model, \"This film is terrible\"))\n","# print(\"For text: \"\"\"\"This film is great\"\"\"\" ->\", predictFIRST(model, \"This film is great\"))\n","# # neg\n","# print(\"For text: \"\"\"\"david carradine sad thai law sure fowl play\"\"\"\" ->\", predictFIRST(model, \"david carradine sad thai law sure fowl play\"))\n","# # pos\n","# print(\"For text: \"\"\"\"tell bro say congrats\"\"\"\" ->\", predictFIRST(model, \"tell bro say congrats\"))\n","# # pos\n","# print(\"For text: \"\"\"\"indeed\"\"\"\" ->\", predictFIRST(model, \"indeed\"))\n","# # pos\n","# print(\"For text: \"\"\"\"completed race life mins girlies work fun\"\"\"\" ->\", predictFIRST(model, \"completed race life mins girlies work fun\"))\n","# def writePreds(row):\n","#   if( round(predictFIRST(model, row))==1 ):\n","#     return \"neg\"\n","#   else:\n","#     return \"pos\"\n","# def computeDFAccuracy(df):\n","#   df['Preds'] = df['text'].apply(lambda x: writePreds(x))\n","#   df['Correct'] = df.apply(lambda x: x['label']==x['Preds'], axis = 1)\n","#   print(df['Correct'].unique())\n","#   print(df.Correct.value_counts()[True])\n","#   print(df.Correct.value_counts())\n","#   perCor = percentage(df.Correct.value_counts()[True], df.shape[0])\n","#   acc = accuracy_score(df['label'], df['Preds'])\n","#   return df, perCor, acc\n","# testTRY1, perCor, acc = computeDFAccuracy(testTRY1)\n","# print(\"Percentage of correct is \", perCor)\n","# print(\"Accuracy is \", acc)\n","# firstACC = acc\n","# end_timeTOTAL = time.time()\n","# mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","# print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5gPMrUmUJ0M"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyFrCI4ZUJxN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVFQpAgOUJun"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifUA9fSyhI3G"},"source":["!pip install torchtext==0.9\n","\n","import torchtext\n","\n","start_timeTOTAL = time.time()\n","\n","TEXT = torchtext.legacy.data.Field(tokenize = 'spacy', preprocessing = generate_bigrams)                   # for second\n","\n","LABEL = torchtext.legacy.data.Field(dtype = torch.float, is_target=True, unk_token=None, batch_first=True)\n","# LABEL = data.LabelField(dtype = torch.float, is_target=True, unk_token=None, batch_first=True)\n","\n","fields = [(None, None), ('label', LABEL), ('text',TEXT)]\n","\n","training_data = torchtext.legacy.data.TabularDataset(\n","# training_data = data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/giannikos/trainTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","\n","# test_data = data.TabularDataset(\n","test_data = torchtext.legacy.data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/giannikos/testTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","\n","for i in range(5):\n","  print(vars(training_data.examples[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkX6UwGIsxc2"},"source":["defaultdict(<function _default_unk_index at 0x7f29e85c8ea0>, {'pos': 0, 'neg': 1})\n","\n","oso pio konta sto 0 pio thetiko\n","oso pio konta sto 1 arnitiko"]},{"cell_type":"code","metadata":{"id":"16jH5aDEhI3G"},"source":["train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of test examples: {len(test_data)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWIPHaRDhI3G"},"source":["#initialize glove embeddings\n","MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(\n","        train_data,\n","        max_size = MAX_VOCAB_SIZE,\n","        vectors = \"glove.6B.100d\",\n","        unk_init = torch.Tensor.normal_,\n","    )\n","\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CK8p58p6hI3G"},"source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n","\n","#Commonly used words\n","print(TEXT.vocab.freqs.most_common(10))\n","\n","untill=0\n","for item in TEXT.vocab.stoi.items():\n","    print(item)\n","    untill+=1\n","    if(untill==10):\n","      break\n","\n","print(TEXT.vocab.itos[:10])\n","print(LABEL.vocab.stoi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BahF444mhI3H"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","BATCH_SIZE = 64\n","\n","#Load an iterator\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","      (train_data, valid_data, test_data), \n","      batch_size = BATCH_SIZE,\n","      sort_key = lambda x: len(x.text),\n","      sort_within_batch=True,\n","      device = device\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoERrYqrhI3H"},"source":["class FastText(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n","        \n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","        \n","    def forward(self, text):\n","        \n","        embedded = self.embedding(text)\n","        embedded = embedded.permute(1, 0, 2)\n","        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n","                \n","        return self.fc(pooled)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J30L3hPOhI3H"},"source":["#define hyperparameters\n","INPUT_DIM       = len(TEXT.vocab)\n","EMBEDDING_DIM   = 100\n","N_FILTERS       = 100\n","HIDDEN_DIM      = 256\n","OUTPUT_DIM      = 1\n","N_LAYERS        = 2\n","BIDIRECTIONAL   = True\n","DROPOUT         = 0.5\n","N_EPOCHS        = 5\n","PAD_IDX         = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1Qban9ZhI3H"},"source":["print(model)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37CiM55hhI3H"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","print(pretrained_embeddings.shape)\n","\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_ZDPbFLhI3H"},"source":["optimizer = optim.Adam(model.parameters())\n","# optimizer = optim.RMSprop(model.parameters())\n","criterion = nn.BCEWithLogitsLoss()\n","# criterion = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9MX5kuWhI3H"},"source":["#push to cuda if available\n","model = model.to(device)\n","criterion = criterion.to(device)\n","\n","#define metric\n","def binary_accuracy(preds, y):\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GlHUnLthI3H"},"source":["There are 2 phases while building the model:\n","\n","* Training phase: model.train() sets the model on the training phase and activates the dropout layers.\n","* Inference phase: model.eval() sets the model on the evaluation phase and deactivates the dropout layers.\n","\n","Note, you do not need to do model.forward(batch.text), simply calling the model works."]},{"cell_type":"code","metadata":{"id":"bzpNA89UhI3H"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()  \n","    \n","    for batch in iterator:\n","        \n","        #resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        predictions = model(batch.text).squeeze(1)              # for third, second\n","        #compute the loss\n","        loss = criterion(predictions, batch.label)        \n","        #compute the binary accuracy\n","        acc = binary_accuracy(predictions, batch.label)   \n","        #backpropage the loss and compute the gradients\n","        loss.backward()       \n","        #update the weights\n","        optimizer.step()      \n","        \n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()    \n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OYvCDaUbhI3H"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    #deactivating dropout layers\n","    model.eval()\n","    \n","    #deactivates autograd\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            \n","            predictions = model(batch.text).squeeze(1)                # for third, second\n","            \n","            #compute loss and accuracy\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            \n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W87NcfCAhI3H"},"source":["we will train the model for a certain number of epochs and save the best model every epoch"]},{"cell_type":"code","metadata":{"id":"XD2X25ZJhI3H"},"source":["best_valid_loss = float('inf')\n","\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","     \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\\n')\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(valid_loss)\n","\n","\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mn_Rss2BhI3H"},"source":["Τhe metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."]},{"cell_type":"markdown","metadata":{"id":"F1Ic3G85hI3H"},"source":["load the best model and define the inference function  that accepts the user defined input and make predictions"]},{"cell_type":"code","metadata":{"id":"WoaBKwMphI3H"},"source":["#load weights\n","path='/content/saved_weights.pt'\n","model.load_state_dict(torch.load(path));\n","\n","test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcwosHx4hI3H"},"source":["# second pred\n","def predictSECOND(model, sentence, flag=False):\n","    model.eval()\n","    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTn-0vWrhI3H"},"source":["print(\"For text: \"\"\"\"This film is terrible\"\"\"\" ->\", predictSECOND(model, \"This film is terrible\"))\n","print(\"For text: \"\"\"\"This film is great\"\"\"\" ->\", predictSECOND(model, \"This film is great\"))\n","# neg\n","print(\"For text: \"\"\"\"david carradine sad thai law sure fowl play\"\"\"\" ->\", predictSECOND(model, \"david carradine sad thai law sure fowl play\"))\n","# pos\n","print(\"For text: \"\"\"\"tell bro say congrats\"\"\"\" ->\", predictSECOND(model, \"tell bro say congrats\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC47B0vQhI3H"},"source":["# for i in range(20, 40, 1):\n","#   print( rejoin_words(vars(test_data.examples[i])['text']), '\\t\\t\\tACTUAL\\t',vars(test_data.examples[i])['label'], 'PREDICTED ',predictSECOND(model, rejoin_words(vars(test_data.examples[i])['text'])), \"\\n\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oifqjh1SnqqI"},"source":["def writePredsSECOND(row):\n","  if( round(predictSECOND(model, row))==1 ):\n","    return \"neg\"\n","  else:\n","    return \"pos\"\n","\n","def computeDFAccuracySECOND(df):\n","\n","  df['Preds'] = df['text'].apply(lambda x: writePredsSECOND(x))\n","  df['Correct'] = df.apply(lambda x: x['label']==x['Preds'], axis = 1)\n","\n","  print(df['Correct'].unique())\n","  print(df.Correct.value_counts()[True])\n","  print(df.Correct.value_counts())\n","\n","  perCor = percentage(df.Correct.value_counts()[True], df.shape[0])\n","  acc = accuracy_score(df['label'], df['Preds'])\n","\n","  return df, perCor, acc\n","\n","testTRY2, perCor, acc = computeDFAccuracySECOND(testTRY2)\n","\n","print(\"Percentage of correct is \", perCor)\n","print(\"Accuracy is \", acc)\n","\n","secondACC = acc\n","\n","testTRY2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvmffkXsUJsx"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3F3NIn2DUJlD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fCbzn__UJfr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CX0tCkHLUJZ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kk1JYMIYz5ya"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DH6BT08_hmbL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoB1ug2pURDU"},"source":["start_timeTOTAL = time.time()\n","TEXT = data.Field(tokenize = 'spacy', batch_first = True)                                   # third\n","\n","LABEL = data.LabelField(dtype = torch.float, is_target=True, unk_token=None, batch_first=True)\n","\n","fields = [(None, None), ('label', LABEL), ('text',TEXT)]\n","\n","training_data = data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/DI/trainTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","\n","test_data = data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/DI/testTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","\n","for i in range(5):\n","  print(vars(training_data.examples[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLkCW02tURDW"},"source":["train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of test examples: {len(test_data)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kE9F4BFGURDX"},"source":["#initialize glove embeddings\n","MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(\n","        train_data,\n","        max_size = MAX_VOCAB_SIZE,\n","        vectors = \"glove.6B.100d\",\n","        unk_init = torch.Tensor.normal_,\n","        # min_freq=4,\n","    )\n","\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oP2MDmoNURDX"},"source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n","\n","print(TEXT.vocab.freqs.most_common(10))\n","\n","untill=0\n","for item in TEXT.vocab.stoi.items():\n","    print(item)\n","    untill+=1\n","    if(untill==10):\n","      break\n","\n","print(TEXT.vocab.itos[:10])\n","print(LABEL.vocab.stoi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sgz_VhoqURDX"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","BATCH_SIZE = 64\n","\n","#Load an iterator\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","      (train_data, valid_data, test_data),\n","      batch_size = BATCH_SIZE,\n","      sort_key = lambda x: len(x.text),\n","      sort_within_batch=True,\n","      device = device\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cROlo3DURDX"},"source":["class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n","                 dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels = 1, \n","                                              out_channels = n_filters, \n","                                              kernel_size = (fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        embedded = embedded.unsqueeze(1)\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        cat = self.dropout(torch.cat(pooled, dim = 1))\n","        return self.fc(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"31nIfNc2URDX"},"source":["#define hyperparameters\n","INPUT_DIM     = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","N_FILTERS     = 100\n","FILTER_SIZES  = [1, 1, 1]\n","HIDDEN_DIM    = 256\n","OUTPUT_DIM    = 1\n","N_LAYERS      = 2\n","BIDIRECTIONAL = True\n","DROPOUT       = 0.5\n","N_EPOCHS      = 5\n","PAD_IDX       = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1dXW8LUURDX"},"source":["print(model)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhIXOJIgURDY"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","\n","print(pretrained_embeddings.shape)\n","\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5pgjg8nURDY"},"source":["optimizer = optim.Adam(model.parameters())\n","# optimizer = optim.RMSprop(model.parameters())\n","\n","criterion = nn.BCEWithLogitsLoss()\n","# criterion = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtWsTfaAURDY"},"source":["model = model.to(device)\n","criterion = criterion.to(device)\n","\n","def binary_accuracy(preds, y):\n","\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUrNzy3zURDY"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        #resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        predictions = model(batch.text).squeeze(1)              # for third\n","        #compute the loss\n","        loss = criterion(predictions, batch.label)        \n","        #compute the binary accuracy\n","        acc = binary_accuracy(predictions, batch.label)   \n","        #backpropage the loss and compute the gradients\n","        loss.backward()       \n","        #update the weights\n","        optimizer.step()   \n","           \n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()    \n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqzmqYEQURDY"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    #deactivating dropout layers\n","    model.eval()\n","    \n","    #deactivates autograd\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            \n","            predictions = model(batch.text).squeeze(1)                # for third\n","            #compute loss and accuracy\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            \n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCVh_zeEURDY"},"source":["we will train the model for a certain number of epochs and save the best model every epoch"]},{"cell_type":"code","metadata":{"id":"wyrWVK7HURDY"},"source":["best_valid_loss = float('inf')\n","\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","     \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\\n')\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(valid_loss)\n","\n","\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAs1GZskURDZ"},"source":["#load weights\n","path='/content/saved_weights.pt'\n","model.load_state_dict(torch.load(path));\n","\n","test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JG9kdLFEURDa"},"source":["def predictTHIRD(model, sentence, flag=False, min_len = 5):\n","    model.eval()\n","\n","    if(flag==False):\n","      tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    else:\n","      tokenized = sentence\n","\n","\n","    if len(tokenized) < min_len:\n","        tokenized += ['<pad>'] * (min_len - len(tokenized))\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(0)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPotVc4jURDa"},"source":["print(\"For text: \"\"\"\"This film is terrible\"\"\"\" ->\", predictTHIRD(model, \"This film is terrible\"))\n","print(\"For text: \"\"\"\"This film is great\"\"\"\" ->\", predictTHIRD(model, \"This film is great\"))\n","# neg\n","print(\"For text: \"\"\"\"david carradine sad thai law sure fowl play\"\"\"\" ->\", predictTHIRD(model, \"david carradine sad thai law sure fowl play\"))\n","# pos\n","print(\"For text: \"\"\"\"tell bro say congrats\"\"\"\" ->\", predictTHIRD(model, \"tell bro say congrats\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amzCfqqfURDa"},"source":["# for i in range(20, 40, 1):\n","#   print( vars(test_data.examples[i])['text'], '\\t\\t\\t\\t\\thave actual\\t',vars(test_data.examples[i])['label'], 'and pred ', predictTHIRD(model, vars(test_data.examples[i])['text'], True)                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHHr61CLpQzL"},"source":["def writePredsTHIRD(row):\n","  if( round(predictTHIRD(model, row))==1 ):\n","    return \"neg\"\n","  else:\n","    return \"pos\"\n","\n","def computeDFAccuracyTHIRD(df):\n","\n","  df['Preds'] = df['text'].apply(lambda x: writePredsTHIRD(x))\n","  df['Correct'] = df.apply(lambda x: x['label']==x['Preds'], axis = 1)\n","\n","  print(df['Correct'].unique())\n","  print(df.Correct.value_counts()[True])\n","  print(df.Correct.value_counts())\n","\n","  perCor = percentage(df.Correct.value_counts()[True], df.shape[0])\n","  acc = accuracy_score(df['label'], df['Preds'])\n","\n","  return df, perCor, acc\n","\n","testTRY2, perCor, acc = computeDFAccuracyTHIRD(testTRY2)\n","\n","print(\"Percentage of correct is \", perCor)\n","print(\"Accuracy is \", acc)\n","\n","thirdACC = acc\n","\n","testTRY2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ulGHL4nCdYD"},"source":["end_timeTOTAL = time.time()\n","\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CVnNPF5es5O"},"source":["# print(\"First ACC\", firstACC)\n","print(\"Second ACC\", secondACC)\n","print(\"Third ACC\", thirdACC)"],"execution_count":null,"outputs":[]}]}
