{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex4_doc2Vec.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQY9SwLkydRz"},"source":["# Γιάννης Δαλιάνης\n","# 1115201700027\n","# Homework 4\n","<!-- # Άσκηση 1 -->\n","\n","<!-- [Source](https://github.com/bentrevett/pytorch-sentiment-analysis) -->"]},{"cell_type":"markdown","metadata":{"id":"PSNvqFp0_cDs"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"yGaeiJ3lyPv0"},"source":["import os\n","import re\n","import pandas as pd\n","import time\n","import json\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from google.colab import drive\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nikl7HNbBTFr"},"source":["## A GPU can be added by going to the menu and selecting:\n","## Runtime -> Change runtime type -> Hardware accelerator: GPU"]},{"cell_type":"code","metadata":{"id":"UnsVXZyFBVaB"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","start_timeTOTAL = time.time()\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBYBPath_g3v"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"id":"CzXaroXcz-BD"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","\n","if os.path.isfile('/content/gdrive/My Drive/giannikos/dataframe.csv'):\n","  print (\"File exist\")\n","  df2 = pd.read_csv('/content/gdrive/My Drive/giannikos/dataframe.csv')\n","  df2 = df2.drop(['Unnamed: 0'], axis=1)\n","else:\n","  print (\"File not exist\")\n","  num = 0\n","  \n","  totalList = list()\n","  for filename in os.listdir('/content/gdrive/My Drive/giannikos/comm_use_subset/'):\n","    rowList = list()\n","    rowList.append(filename)\n","    if num%300 == 0:\n","      print(num)\n","    with open('/content/gdrive/My Drive/giannikos/comm_use_subset/' + filename) as json_file:\n","        wholeText = \"\"\n","        data = json.load(json_file)\n","        \n","        article_title = data['metadata']['title']\n","        \n","        abstract = list()\n","        abstract.append(article_title)\n","        for p in data['abstract']:\n","          abstract.append(p['text'])\n","          wholeText += p['text']\n","          wholeText += \" \"\n","        \n","        body_text = list()\n","        for p in data['body_text']:\n","          body_text.append(p['text'])\n","          wholeText += p['text']\n","          wholeText += \" \"\n","        \n","        rowList.append(article_title)\n","        rowList.append(wholeText)\n","        num += 1\n","    \n","        rowList.append(article_title + \". \" + wholeText)\n","        comb = abstract + body_text\n","        \n","        rowList.append(comb)\n","\n","    totalList.append(rowList)\n","  \n","  df2 = pd.DataFrame(totalList, columns=['fileName', 'fileTitle', 'fileText', 'combinedTitle_and_Text', 'paragraphs'])\n","  df2.to_csv(r'/content/gdrive/My Drive/DI/Colab Notebooks/ex4/dataframe.csv')\n","\n","df2['fileTitle'] = df2['fileTitle'].astype(str)\n","df2['fileText'] = df2['fileText'].astype(str)\n","df2['combinedTitle_and_Text'] = df2['combinedTitle_and_Text'].astype(str)\n","# df2 = df2[:6000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XM_dfXbbh9TR"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"HCvsFiLoBq1L"},"source":["Text preprocessing isn't essential, because we are working on scientific papers and almost every word, number etc are important for the content of the papers. Urls and special characters have been removed."]},{"cell_type":"markdown","metadata":{"id":"Kqgr7TNCH4JA"},"source":["Saves paragraphs as string instead of list so use literal_eval."]},{"cell_type":"code","metadata":{"id":"kDZem3M4h-4w"},"source":["def cleanText(text):\n","    text = text.str.replace(r'RT[\\s]+', '')                                             # Removing RT\n","    text = text.str.replace(r'#.*?(?=\\s|$)', '')                                        # remove hashtags and mentions\n","    text = text.str.replace(r'@.*?(?=\\s|$)', '')\n","    text = text.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)  # remove urls\n","    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n","    text = text.str.replace(r\"\\'re\", \" are\")                                            # Change 're to 'are'\n","    text = text.str.replace(r\"\\'t\", \" not\")                                             # Change 't to 'not'\n","    text = text.str.replace(r\"\\'d\", \" would\")                                           # Change 'd to 'would'\n","    text = text.replace(r'\\\\n',' ', regex=True)                                         # remove newlines and other special characters\n","    text = text.replace(r'\\\\u',' ', regex=True)\n","    text = text.replace(r'\\\\x',' ', regex=True)\n","    return text\n","\n","df2['fileTitle'] = cleanText(df2['fileTitle'])\n","df2['fileText'] = cleanText(df2['fileText'])\n","df2['combinedTitle_and_Text'] = cleanText(df2['combinedTitle_and_Text'])\n","\n","df2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQjD_v_iCcP1"},"source":["## Querries"]},{"cell_type":"code","metadata":{"id":"6Tokh5zSCf2W"},"source":["quers = [\n","    \"What are the coronoviruses?\",\n","    \"What was discovered in Wuhuan in December 2019?\",\n","    \"What is Coronovirus Disease 2019?\",\n","    \"What is COVID-19?\",\n","    \"What is caused by SARS-COV2?\",\n","    \"How is COVID-19 spread?\",\n","    \"Where was COVID-19 discovered?\",\n","    \"How does coronavirus spread?\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jFl1Stwktal"},"source":["## Implementation description"]},{"cell_type":"markdown","metadata":{"id":"vowBr6d_k0By"},"source":["The 2 embedding technics used are doc2vec and SBert. For each one, we identify papers similar to our question based on titles only and based on titles combined with corpus as well. 'Paragraphs' column is used for returning passages that answer our querries. The QA bert model doesn't work with large corpora. "]},{"cell_type":"markdown","metadata":{"id":"qLoKucNFVZb0"},"source":["Results are better when we train our models on large text corpora, because scientific papers are very large, so they should be thoroughly examined as it comes to their similarity with the given question."]},{"cell_type":"markdown","metadata":{"id":"QRsTGNzxVTVZ"},"source":["SBert works better than doc2vec because it follows a context-dependet word embedding method which takes into account the content of the papers more clearly. SBert is also by far quicker than doc2vec. As Sbert is context-dependet, it returns more similar papers. SBert returns more suitable papers for the most of our questions. "]},{"cell_type":"markdown","metadata":{"id":"6eZ50LZAVcQ6"},"source":["When using the full corpora, we print more indicative passages to see how good our results are. Especially SBert takes into account words of the same semantic family."]},{"cell_type":"markdown","metadata":{"id":"9KgMru8iVGTN"},"source":["## doc2vec on titles only"]},{"cell_type":"markdown","metadata":{"id":"7uKAYc8t8g-A"},"source":["The word2vec gives a numeric representation for each word, that will be able to capture relations between words such as Paris and France. This is part of a wider concept in machine learning — the feature vectors."]},{"cell_type":"markdown","metadata":{"id":"t3HSAE24jaIg"},"source":["An extension of Word2Vec, the Doc2Vec embedding. Build a tagged sentence corpus. Each sentence is now represented as a TaggedDocument containing a list of the words in it and a tag associated with it. Our text needs to have been tokenized."]},{"cell_type":"code","metadata":{"id":"TFp-npekYcYI"},"source":["df2[\"toked\"] = df2[\"fileTitle\"].apply(word_tokenize)\n","\n","tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(df2[\"toked\"])]\n","tagged_data[:6]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29x0sCZreRht"},"source":["## Train doc2vec model\n","model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n","\n","## Print model vocabulary\n","num=0\n","for i, j in ((model.wv.vocab).items()):\n","  if num>5:\n","    break\n","  print(i, j)\n","  num+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyHQm5vujVra"},"source":["for qSent in quers:\n","  test_doc = word_tokenize(qSent)\n","\n","  test_doc_vector = model.infer_vector(test_doc)\n","\n","  ret = model.docvecs.most_similar(positive = [test_doc_vector])\n","\n","  print(\"\\n\\n======================\\n\\n\")\n","  print(\"Query:\", qSent)\n","  print(\"\\nTop 5 most similar text:\")\n","\n","  for i in (range(5)):\n","    print( \"Similarity\", ret[i][1] )\n","    print( \"Article Title:\\n\", df2['fileTitle'].iloc[ret[i][0]])\n","    # print( \"Article Body:\\n\", df2['fileText'].iloc[ret[i][0]])\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_3w8hQkmd0d"},"source":["## doc2vec on combined Titles and Corpus"]},{"cell_type":"code","metadata":{"id":"1AsNbzJPmd0m"},"source":["df2[\"toked1\"] = df2[\"combinedTitle_and_Text\"].apply(word_tokenize)\n","\n","tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(df2[\"toked1\"])]\n","tagged_data[:6]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKhHakjkmd0n"},"source":["## Train doc2vec model\n","model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n","\n","num=0\n","for i, j in ((model.wv.vocab).items()):\n","  if num>5:\n","    break\n","  print(i, j)\n","  num+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jr4HNVJJmd0n"},"source":["for qSent in quers:\n","  test_doc = word_tokenize(qSent)\n","\n","  test_doc_vector = model.infer_vector(test_doc)\n","\n","  ret = model.docvecs.most_similar(positive = [test_doc_vector])\n","\n","  print(\"\\n\\n======================\\n\\n\")\n","  print(\"Query:\", qSent)\n","  print(\"\\nTop 5 most similar text:\")\n","\n","  for i in (range(5)):\n","    print( \"Similarity\", ret[i][1] )\n","    print( \"Article Title:\\n\", df2['fileTitle'].iloc[ret[i][0]])\n","    # print( \"Article Body:\\n\", df2['fileText'].iloc[ret[i][0]])\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErTc9OzqGS44"},"source":["## Time Needed"]},{"cell_type":"code","metadata":{"id":"VvmffkXsUJsx"},"source":["end_timeTOTAL = time.time()\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]}]}