{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex3_RNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaidpj2/mIdWfo0FfWSHhn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQY9SwLkydRz"},"source":["# Γιάννης Δαλιάνης\n","# 1115201700027\n","# Homework 3\n","# Άσκηση 1\n","\n","[Source](https://github.com/bentrevett/pytorch-sentiment-analysis)"]},{"cell_type":"markdown","metadata":{"id":"PSNvqFp0_cDs"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"yGaeiJ3lyPv0"},"source":["import os\n","import re\n","import pandas as pd\n","import time\n","import random\n","import numpy as np\n","import nltk\n","import spacy\n","from google.colab import drive\n","from matplotlib import pyplot as plt\n","nltk.download('stopwords')\n","nlp = spacy.load('en')\n","from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score, classification_report\n","import torch   \n","from torchtext import data, datasets\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBYBPath_g3v"},"source":["## Load Dataset and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"V92N4VW11E0w"},"source":["Drop uneccessary columns."]},{"cell_type":"code","metadata":{"id":"O-2jcppmym1C"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","tweets = pd.read_csv('/content/gdrive/My Drive/DI/Colab Notebooks/ex3/SentimentTweets.csv')\n","print(tweets.columns)\n","tweets = tweets.drop(['Unnamed: 0', 'id', 'date', 'flag', 'user'], axis=1)\n","tweets.rename(columns={'target': 'label'}, inplace = True)\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3mTXQrv0gZu"},"source":["tweets = tweets.sample(n = 3000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVlEnWZ41Hvj"},"source":["Clean tweet text, remove stopwords and save to new csv file."]},{"cell_type":"code","metadata":{"id":"SnpvViu01Icg"},"source":["tweets = tweets.apply(lambda x: x.astype(str).str.lower())\n","def cleanText(text):\n","    text = text.str.replace(r'RT[\\s]+', '')                                              # Removing RT\n","    text = text.str.replace(r'&amp;', '&')            # Replace '&amp;' with '&'\n","    text = text.str.replace(r'#.*?(?=\\s|$)', '')                                        # remove hashtags and mentions\n","    text = text.str.replace(r'@.*?(?=\\s|$)', '')\n","    text = text.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)  # remove urls\n","    text = text.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n","    text = text.str.replace(r\"\\'re\", \" are\")                                             # Change 're to 'are'\n","    text = text.str.replace(r\"\\'t\", \" not\")                                             # Change 't to 'not'\n","    text = text.str.replace(r\"\\'d\", \" would\")                                           # Change 'd to 'would'\n","    text = text.replace(r'\\\\n',' ', regex=True)                                         # remove newlines and other special characters\n","    text = text.replace(r'\\\\u',' ', regex=True)\n","    text = text.replace(r'\\\\x',' ', regex=True)\n","    text = text.str.replace('\\d+', '')                                                  # remove all numbers\n","    text = text.str.replace(r'\\b(\\w{1,2})\\b', '')                                       # remove words with 2 or 1 letter only\n","    text = text.str.replace('[^\\w\\s]','')                                               # remove punctuations\n","    text = text.apply(lambda x: re.sub(' +', ' ', x))                                   # replace multiple whitespaces\n","    # text = text.str.replace(r'(@.*?)[\\s]', '')        # Remove '@name'\n","    # text = text.str.replace(r'\\s+', ' ').str.strip()  # Remove trailing whitespace\n","    return text\n","def remove_stops(row):\n","  stops = nltk.corpus.stopwords.words(\"english\")\n","  meaningful_words = [w for w in row if w not in stops]\n","  return meaningful_words\n","def rejoin_words(row):\n","  joined_words = ( \" \".join(row))\n","  return joined_words\n","\n","tweets['text'] = cleanText(tweets['text'])\n","\n","# erase empty lines\n","nan_value = float(\"NaN\")\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","# 0 will be for negative and 1 for positive\n","tweets['label'] = tweets['label'].astype(int)\n","tweets['label'].replace(4, 1, inplace=True)\n","\n","# # no good for for Sentiment Classification and takes some time\n","# tweets[\"text\"] = tweets[\"text\"].str.split()\n","# tweets['text'] = tweets['text'].apply(lambda x: remove_stops(x))\n","# tweets['text'] = tweets['text'].apply(lambda x: rejoin_words(x))\n","\n","nan_value = float(\"NaN\")  # erase empty lines\n","tweets['text'].replace(\" \", \"\", inplace=True)\n","tweets['text'].replace(\"\", nan_value, inplace=True)\n","tweets.dropna(subset=['text'], inplace=True)\n","\n","tweets.reset_index(drop = True, inplace = True)\n","\n","tweets[\"label\"].replace( { 1: \"pos\", 0: \"neg\" }, inplace=True )\n","\n","trainTWEETS, testTWEETS = train_test_split(tweets, test_size=0.2)\n","print(\"Value counts for Train sentiments\")\n","print(trainTWEETS.label.value_counts())\n","print(\"Value counts for Test sentiments\")\n","print(testTWEETS.label.value_counts())\n","trainTWEETS.to_csv(r'/content/gdrive/My Drive/DI/Colab Notebooks/ex3/trainTWEETS.csv')\n","testTWEETS.to_csv(r'/content/gdrive/My Drive/DI/Colab Notebooks/ex3/testTWEETS.csv')\n","tweets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWTiKmgYAsrP"},"source":["## Test Data Set"]},{"cell_type":"code","metadata":{"id":"S0NhAfXggCmS"},"source":["testTRY = pd.read_csv('/content/gdrive/My Drive/DI/Colab Notebooks/ex3/testTWEETS.csv', index_col=0)\n","testTRY"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nikl7HNbBTFr"},"source":["## A GPU can be added by going to the menu and selecting:\n","## Runtime -> Change runtime type -> Hardware accelerator: GPU"]},{"cell_type":"code","metadata":{"id":"UnsVXZyFBVaB"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","start_timeTOTAL = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tgPr58n7BXuv"},"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1JKEkY9BNdk"},"source":["## RNN"]},{"cell_type":"markdown","metadata":{"id":"4jQzuYRj3Q52"},"source":["An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$. \n","\n","$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n","\n","We'll be using an RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple _gates_ which control the flow of information into and out of the memory. We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n","\n","$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n","\n","The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n","\n","The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n","\n","In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n","\n","We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$\n","\n","Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n","\n","The more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing  a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called *dropout*. Dropout works by randomly *dropping out* (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit."]},{"cell_type":"code","metadata":{"id":"Y7v7kaD6dCIj"},"source":["SEED = 1234\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifUA9fSyhI3G"},"source":["TEXT  = data.Field(tokenize = 'spacy', include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float, is_target=True, unk_token=None, batch_first=True)\n","\n","fields = [(None, None), ('label', LABEL), ('text',TEXT)]\n","training_data = data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/DI/Colab Notebooks/ex3/trainTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","test_data = data.TabularDataset(\n","                          path =  '/content/gdrive/My Drive/DI/Colab Notebooks/ex3/testTWEETS.csv',\n","                          format = 'csv',\n","                          fields = fields,\n","                          skip_header = True\n","                  )\n","for i in range(5):\n","  print(vars(training_data.examples[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16jH5aDEhI3G"},"source":["train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of test examples: {len(test_data)}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mxyuen80FBev"},"source":["These pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch."]},{"cell_type":"code","metadata":{"id":"GWIPHaRDhI3G"},"source":["#initialize glove embeddings\n","MAX_VOCAB_SIZE = 35_000 # top words\n","TEXT.build_vocab(\n","        train_data,\n","        max_size = MAX_VOCAB_SIZE,\n","        vectors = \"glove.6B.100d\",\n","        unk_init = torch.Tensor.normal_,\n","    )\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opQqHZfxX85o"},"source":["Sometimes takes neg as 0 and sometimes takes neg as 1."]},{"cell_type":"code","metadata":{"id":"CK8p58p6hI3G"},"source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n","\n","#Commonly used words\n","print(TEXT.vocab.freqs.most_common(20))\n","print(TEXT.vocab.itos[:20])\n","print(LABEL.vocab.stoi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkX6UwGIsxc2"},"source":["two possibilities:\n","\n","defaultdict(<function _default_unk_index at 0x7f588bf2eae8>, {'neg': 0, 'pos': 1})\n","\n","defaultdict(<function _default_unk_index at 0x7f588bf2eae8>, {'neg': 1, 'pos': 0})"]},{"cell_type":"code","metadata":{"id":"BahF444mhI3H"},"source":["#define hyperparameters\n","INPUT_DIM       = len(TEXT.vocab)\n","EMBEDDING_DIM   = 100\n","HIDDEN_DIM      = 300\n","OUTPUT_DIM      = 1\n","N_LAYERS        = 2\n","BIDIRECTIONAL   = True\n","DROPOUT         = 0.5\n","N_EPOCHS        = 50\n","PAD_IDX         = TEXT.vocab.stoi[TEXT.pad_token]\n","BATCH_SIZE      = 256\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","      (train_data, valid_data, test_data), \n","      batch_size = BATCH_SIZE,\n","      sort_key = lambda x: len(x.text),\n","      sort_within_batch=True,\n","      device = device\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYjCljjwvvzg"},"source":["trainloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGnM1dML7-pJ"},"source":["Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n","\n","The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space.\n","\n","The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n","\n","Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n","\n","We feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."]},{"cell_type":"code","metadata":{"id":"JoERrYqrhI3H"},"source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","                \n","        self.dropout = nn.Dropout(p=dropout)\n","    \n","    def forward(self, text, text_lengths):\n","        embedded = self.dropout(self.embedding(text))\n","        \n","        # pack sequence\n","        if(torch.cuda.is_available()):\n","            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths = text_lengths.cpu())\n","        else:\n","            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        # unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","        return self.fc(hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J30L3hPOhI3H"},"source":["model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)\n","print(model)\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0uYg_-REmuE"},"source":["We'll be using *packed padded sequences*, which will make our RNN only process the non-padded elements of our sequence, and for any padded element the `output` will be a zero tensor.\n","\n","Copying the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n","\n","We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n","\n","As our `<unk>` and `<pad>` token aren't in the pre-trained vocabulary they have been initialized using `unk_init` (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment. \n","\n","We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index."]},{"cell_type":"code","metadata":{"id":"37CiM55hhI3H"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","print(pretrained_embeddings.shape)\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9MX5kuWhI3H"},"source":["optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","print(optimizer)\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n","\n","#define metric\n","def binary_accuracy(preds, y):\n","    # Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzpNA89UhI3H"},"source":["def train(model, iterator, optimizer, criterion):\n","    epoch_loss  = 0\n","    epoch_acc   = 0\n","    model.train()\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","        text, text_lengths = batch.text\n","\n","        predictions = model(text, text_lengths).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 4.0)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OYvCDaUbhI3H"},"source":["def evaluate(model, iterator, criterion):\n","    epoch_loss  = 0\n","    epoch_acc   = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","            text, text_lengths = batch.text\n","            predictions = model(text, text_lengths).squeeze(1)\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lL5vWCfJ93Z"},"source":["Early Stoping Implementation from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py"]},{"cell_type":"code","metadata":{"id":"JbMS7aTWI1gN"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved. Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0\n","            path (str): Path for the checkpoint to be saved to. Default: 'checkpoint.pt'\n","            trace_func (function): trace print function. Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD2X25ZJhI3H"},"source":["train_losses  = []\n","val_losses    = []\n","train_accL  = []\n","val_accL    = []\n","iter = []\n","\n","early_stopping = EarlyStopping(patience=4, verbose=True)\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. Acc: {valid_acc*100:.2f}%\\n')\n","    \n","    train_losses.append(train_loss)\n","    val_losses.append(valid_loss)\n","    train_accL.append(train_acc)\n","    val_accL.append(valid_acc)\n","    iter.append(epoch)\n","\n","    # early_stopping needs the validation loss to check if it has decresed, \n","    # and if it has, it will make a checkpoint of the current model\n","    early_stopping(valid_loss, model)\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8UH3JsqqM8b"},"source":["If no early stopping used, red line is on last epoch"]},{"cell_type":"code","metadata":{"id":"yYd-q0svKTmz"},"source":["fig = plt.figure(figsize=(10,8))\n","\n","plt.plot(range(1,len(train_losses)+1),train_losses, label='Training Loss')\n","plt.plot(range(1,len(val_losses)+1),val_losses,label='Validation Loss')\n","\n","minposs = val_losses.index(min(val_losses))+1\n","minposs = val_losses.index(min(val_losses))+1\n","plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n","\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIGel1JG4bXx"},"source":["plt.figure(figsize=(10,4))\n","plt.subplot(1,2,1)\n","plt.title(\"Training Curve\")\n","plt.plot(iter, train_losses, label=\"Train\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss\")\n","\n","plt.subplot(1,2,2)\n","plt.title(\"Accuracy Curve\")\n","plt.plot(iter, train_accL, label=\"Train\")\n","plt.plot(iter, val_accL, label=\"Validation\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Training Accuracy\")\n","plt.legend(loc='best')\n","plt.show()\n","\n","print(\"Final Training Accuracy: {}\".format(train_accL[-1]))\n","print(\"Final Validation Accuracy: {}\".format(val_accL[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoaBKwMphI3H"},"source":["test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcwosHx4hI3H"},"source":["def predictFIRST(model, sentence, flag=False):\n","    model.eval()\n","    if(flag==False):\n","      tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    else:\n","        tokenized = sentence\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","\n","    prediction = torch.sigmoid( model(tensor, length_tensor) )\n","    \n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTn-0vWrhI3H"},"source":["PRED_VAL = predictFIRST(model, \"This film is terrible\")\n","print(\"For text: \"\"\"\"This film is terrible\"\"\"\" ->\", PRED_VAL)\n","PRED_VAL = round( PRED_VAL )  # for negative\n","print( PRED_VAL )\n","print(\"For text: \"\"\"\"This film is great\"\"\"\" ->\", predictFIRST(model, \"This film is great\"))\n","# neg\n","print(\"For text: \"\"\"\"david carradine sad thai law sure fowl play\"\"\"\" ->\", predictFIRST(model, \"david carradine sad thai law sure fowl play\"))\n","# pos\n","print(\"For text: \"\"\"\"tell bro say congrats\"\"\"\" ->\", predictFIRST(model, \"tell bro say congrats\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NF4PoiTTOeAM"},"source":["def evaluate_roc(probs, y_true):\n","    \"\"\"\n","    - Print AUC and accuracy on the test set\n","    - Plot ROC\n","    \"\"\"\n","    fpr, tpr, threshold = roc_curve(y_true, probs)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","    \n","    # Plot ROC AUC\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oifqjh1SnqqI"},"source":["def writePredsSECOND(row):\n","  if( round( row )==PRED_VAL ):\n","    return \"neg\"\n","  else:\n","    return \"pos\"\n","def writeProbs(row):\n","  return predictFIRST(model, row)\n","def ProbsToPreds(row):\n","  return round( row )\n","def LabelTOINT(row):\n","  if( row==\"neg\" ):\n","    return PRED_VAL\n","  else:\n","    return 1 - PRED_VAL\n","\n","def computeDFAccuracySECOND(df):\n","  df['Probs'] = df['text'].apply(lambda x: writeProbs(x))\n","  df['Preds'] = df['Probs'].apply(lambda x: writePredsSECOND(x))\n","  df['PredsINTS'] = df['Probs'].apply(lambda x: ProbsToPreds(x))\n","  df['labelINT'] = df['label'].apply(lambda x: LabelTOINT(x))\n","\n","  print(classification_report(df['label'],df['Preds']))\n","  print(\"Accuracy Score -> \",   accuracy_score(   df['label'], df['Preds']))\n","  print(\"Precision Score -> \",  precision_score(  df['label'], df['Preds'], average='macro')*100)  # warnings for small epochs\n","  print(\"Recall Score -> \",     recall_score(     df['label'], df['Preds'], average='macro')*100)\n","  print(\"F-Measure Score -> \",  f1_score(         df['label'], df['Preds'], average='macro')*100)\n","  \n","  evaluate_roc(df['Probs'], df['labelINT']) # thelei labels int\n","  \n","  return df\n","\n","testTRY = computeDFAccuracySECOND(testTRY)\n","testTRY"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErTc9OzqGS44"},"source":["## Time Needed"]},{"cell_type":"code","metadata":{"id":"VvmffkXsUJsx"},"source":["end_timeTOTAL = time.time()\n","mins, secs = epoch_time(start_timeTOTAL, end_timeTOTAL)\n","print(f'Total Time: {mins}m {secs}s')"],"execution_count":null,"outputs":[]}]}